{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.0.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.8.3 (default, Jul  2 2020 17:30:36)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pyspark import *\n",
    "from pyspark import SparkContext as sc, SparkConf\n",
    "from pyspark.python.pyspark.shell import spark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import udf, to_date, to_utc_timestamp, lit, col\n",
    "from pyspark.sql.types import StringType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Sparkcontext and SQL context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import pyspark.sql.functions\n",
    "from pyspark.sql import SQLContext\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_con = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sql_con.read.csv('Amazon_Responded_Oct05.csv', header=True)\n",
    "df_amzn = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-----------+\n",
      "|    tweet_created_at|user_screen_name|user_id_str|\n",
      "+--------------------+----------------+-----------+\n",
      "|Tue Nov 01 01:57:...|     SeanEPanjab|  143515471|\n",
      "|Tue Nov 01 02:39:...|      AmazonHelp|   85741735|\n",
      "|Tue Nov 01 17:14:...|     SeanEPanjab|  143515471|\n",
      "|Tue Nov 01 17:15:...|     SeanEPanjab|  143515471|\n",
      "|Tue Nov 01 17:19:...|      AmazonHelp|   85741735|\n",
      "|Tue Nov 01 17:25:...|      AmazonHelp|   85741735|\n",
      "|Tue Nov 01 17:55:...|     SeanEPanjab|  143515471|\n",
      "|Tue Nov 01 17:55:...|     SeanEPanjab|  143515471|\n",
      "|Tue Nov 01 18:02:...|      AmazonHelp|   85741735|\n",
      "|Tue Nov 01 03:51:...|   aakashwangnoo|   71457972|\n",
      "+--------------------+----------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select('tweet_created_at', 'user_screen_name', 'user_id_str')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1 -  Finding out users who are active in at least five listed days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_split = pyspark.sql.functions.split(df['tweet_created_at'],' ') \n",
    "sql_func = pyspark.sql.functions \n",
    "df = df.withColumn('Month', date_split.getItem(1))\n",
    "df = df.withColumn('Day',date_split.getItem(2))\n",
    "df = df.withColumn('year',date_split.getItem(5))\n",
    "df = df.withColumn('Date',sql_func.concat('Month','Day',lit(\" \"),'year'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-----------+----------+\n",
      "|    tweet_created_at|user_screen_name|user_id_str|      Date|\n",
      "+--------------------+----------------+-----------+----------+\n",
      "|Tue Nov 01 01:57:...|     SeanEPanjab|  143515471|Nov01 2016|\n",
      "|Tue Nov 01 02:39:...|      AmazonHelp|   85741735|Nov01 2016|\n",
      "|Tue Nov 01 17:14:...|     SeanEPanjab|  143515471|Nov01 2016|\n",
      "|Tue Nov 01 17:15:...|     SeanEPanjab|  143515471|Nov01 2016|\n",
      "|Tue Nov 01 17:19:...|      AmazonHelp|   85741735|Nov01 2016|\n",
      "+--------------------+----------------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select('tweet_created_at','user_screen_name','user_id_str','Date')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_cols = df.select('user_id_str','user_screen_name','Date').groupBy(\"user_id_str\",\"user_screen_name\").agg(countDistinct(\"Date\").alias('count')).filter(column('count') >= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+\n",
      "|user_screen_name|user_id_str|\n",
      "+----------------+-----------+\n",
      "|      jeujeubeee| 2214869341|\n",
      "|        nadu_bda| 2219985456|\n",
      "|  jordanwaring87|   47584774|\n",
      "|          sng628|  100613504|\n",
      "|    LalwaniNiraj| 1666520694|\n",
      "|   urvashi_mitra|  527489415|\n",
      "| roadshowrigolet|  630450707|\n",
      "|   ItsJustMe_Rae| 3009368979|\n",
      "|     bijender393|  903272676|\n",
      "|   DadaSiddharth|  359869433|\n",
      "+----------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily_active_users = filter_cols.select('user_screen_name','user_id_str')\n",
    "daily_active_users.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2 - Creating a datafram 'experiment_user' to capture the selected user_id_str and see if they are active users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "expt = open('experiment', \"r\")\n",
    "for item in expt:\n",
    "   new_list.append(item.rstrip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     value|\n",
      "+----------+\n",
      "| 143515471|\n",
      "|  85741735|\n",
      "|  71457972|\n",
      "|2908108256|\n",
      "| 106799492|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_exp = sql_con.createDataFrame(new_list,StringType())\n",
    "df_exp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating temporary table/views\n",
    "----------------------------------------------------\n",
    "\n",
    "table 1 - dataframe consisting of daily active users\n",
    "table 2 - dataframe consisting of list of users in the experiment.txt file\n",
    "table 3 - dataframe consisting of active users among the ones chosen for the AB-test experiment (created later)\n",
    "table 4 - original dataframe of Amazon_Responded_Oct05.csv file (created later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_active_users.createOrReplaceTempView('table1')\n",
    "df_exp.createOrReplaceTempView('table2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_user = spark.sql(\"SELECT value from table2 where value in (SELECT user_id_str FROM table1)\")\n",
    "exp_user = exp_user.withColumn(\"Whether_active\", lit(\"Yes\"))\n",
    "exp_user.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|     value|Whether_active|\n",
      "+----------+--------------+\n",
      "|  85741735|           Yes|\n",
      "| 902137872|           Yes|\n",
      "| 110354554|           Yes|\n",
      "|3285473358|           Yes|\n",
      "|1399965709|           Yes|\n",
      "|  23019151|           Yes|\n",
      "|2698629504|           Yes|\n",
      "|  71302070|           Yes|\n",
      "| 180361172|           Yes|\n",
      "| 493941380|           Yes|\n",
      "+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display the active users from the experiment file \n",
    "exp_user.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the percentage of active users in the new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = df_amzn.count() #total users\n",
    "active_ones = exp_user.count() #active users in those chosen for the experiment\n",
    "\n",
    "#compute percentage of active users\n",
    "percentage = active_ones/total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total percentage of active users:  2.3004600920184037\n"
     ]
    }
   ],
   "source": [
    "print(\"Total percentage of active users: \" , percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3 - Create a new dataframe by joining the three tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create another temporary table with results\n",
    "exp_user.createOrReplaceTempView('table3')\n",
    "\n",
    "#create temporary table for the original Amazon_Responded_Oct05.csv file dataset file\n",
    "df_amzn.createOrReplaceTempView('table4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining the table1, table2 and table 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_join = spark.sql(\"SELECT * FROM table4 where user_id_str in (Select user_id_str FROM table1) AND user_id_str in (SELECT value FROM table2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the resulting dataframe\n",
    "output_file = final_join.toPandas()\n",
    "output_file.to_csv('Amazon_new.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
